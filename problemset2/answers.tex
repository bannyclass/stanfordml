\documentclass[11pt]{article}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage[latin1]{inputenc}
\usepackage[margin=0.5in]{geometry}

\everymath{\displaystyle}
\setlength\parindent{0pt}

\begin{document}
\title{Stanford CS 229, Public Course, Problem Set 2}
\date{\today}
\author{Dylan Price}
\maketitle 

% custom commands
\newcommand{\half}[0]{\frac{1}{2}}
\newcommand{\pder}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\thetaT}[0]{\theta^{T}}
\newcommand{\ith}[1]{#1^{(i)}}
\newcommand{\xith}[0]{\ith{x}}
\newcommand{\yith}[0]{\ith{y}}
\newcommand{\sumitom}[0]{\sum_{i=1}^{m}}
\newcommand{\xnew}[0]{x_{new}}

\section{}

\subsection*{a)}

Find a closed-form expreesion for the value of $\theta$ which minimizes the ridge regression cost function:

$J(\theta) = \half \sumitom (\thetaT \xith - \yith)^{2} + \frac{\lambda}{2}\|\theta\|^{2}$ \\ \\

First, put $J(\theta)$ into matrix notation
\begin{align*}
  J(\theta) &= \half \sumitom (\thetaT \xith - \yith)^{2} + \frac{\lambda}{2}\|\theta\|^{2} \\
            &= \half \sumitom (\thetaT \xith - \yith)(\thetaT \xith - \yith) + \frac{\lambda}{2}\|\theta\|^{2} \\
            &= \half (X \theta - \vec{y})(X \theta - \vec{y}) + \frac{\lambda}{2} \thetaT \theta \\
            & \text{(where $X$ is the design matrix and $\vec{y}$ is the vector of target values)} \\
            &= \half ((X \theta)^{T} X \theta - (X \theta)^{T} \vec{y} - \vec{y}^{T} X \theta + \vec{y}^{T} \vec{y})) + \frac{\lambda}{2} \thetaT \theta \\
            &= (\half \thetaT X^{T} X \theta - \half \thetaT X^{T} \vec{y} - \half \vec{y}^{T} X \theta + \half \vec{y}^{T} \vec{y}) + \frac{\lambda}{2} \thetaT \theta \\
\end{align*}


Now find the gradient
\begin{align*}
  \nabla_{\theta} J(\theta) &= X^{T} X \theta - \half X^{T} \vec{y} - \half X^{T} \vec{y} + \lambda \theta \\
  &= X^{T} X \theta - X^{T} \vec{y} + \lambda \theta \\
\end{align*}

Set the gradient equal to $0$ and solve for $\theta$
\begin{align*}
  X^{T} X \theta - X^{T} \vec{y} + \lambda \theta &= 0 \\
  X^{T} X \theta + \lambda \theta &= X^{T} \vec{y} \\
  (X^{T} X + \lambda I) \theta &= X^{T} \vec{y} \\
  \theta &= (X^{T} X + \lambda I)^{-1} X^{T} \vec{y} \\
\end{align*}

\subsection*{b)}

Suppose that we want to use kernels to implicitly represent our feature vectors in a high-dimensional (possibly infinite dimensional) space. 

Making a prediction on a new input $\xnew$ would now be done by computing $\thetaT \phi(\xnew)$.

Show how we can use the "kernel trick" to obtain a closed form for the prediction on the new input without ever explicitly computing $\phi(\xnew)$.

\begin{align*}
  h_{\theta}(\xnew) &= \thetaT \xnew \\
             &= ((X^{T} X + \lambda I)^{-1} X^{T} \vec{y})^{T} \xnew \\
             &= ((X X^{T} + \lambda I)^{-1} \vec{y})^{T} X \xnew \\
             &= \vec{y}^{T} (X X^{T} + \lambda I)^{-T} X \xnew \\
\end{align*}

Now we replace $\xith$ with $\phi(\xith)$ for $i = 1...m$,\\
define the kernel function $K(x, z) = \phi(x) \phi(z)$ \\
and kernel matrix $K \in \mathbb{R}^{m x m}$ such that $K_{ij} = K(\xith, x^{(j)})$

\begin{align*}
  h_{\theta}(\xnew) &= \vec{y}^{T} (K + \lambda I)^{-T} \sumitom K(\xith, \xnew) \\
\end{align*}

\section*{2}

\begin{align*}
  \underset{w,b,\xi}{\text{min}} &\quad \half \|w\|^{2} + \frac{C}{2} \sumitom \xi_{i}^{2} \\
  \text{subject to} &\quad \yith (w^{T} \xith + b) \geq 1 - \xi_i, i = 1,...,m \\
\end{align*}

\subsection*{a}

Notice that we have dropped the $\xi_{i} \geq 0$ constraint in the $\ell_2$ problem. Show that these non-negativity constraints can be removed. That is, show that the optimal value of the objective will be the same whether or not these constraints are present. \\

Let $\xi_i$ be a negative error in $\xi_0,...,\xi_m$. Since $\xi_i^{2}$ is positive, $\xi_i$ will contribute the same amount to the objective as $-\xi_i$. Therefore the minimization of the objective does not depend on the sign of $\xi_i$ and the $\xi_i \geq 0$ constraint is irrelevant.

\subsection*{b}

What is the Lagrangian of the $\ell_2$ soft margin SVM optimization problem?

Let
\begin{align*}
  f(w)   &= \half \|w\|^{2} + \frac{C}{2} \sumitom \xi_i^2 \\
  g_i(w) &= -\yith (w^T \xith + b) + 1 - \xi_i \\
\end{align*}

Our optimization problem is
\begin{align*}
  \text{min}        &\quad f(w) \\
  \text{subject to} &\quad g_i(w) \geq 0, i = 1,...,m \\
\end{align*}

The Lagrangian is
\begin{align*}
  \mathcal{L}(w,b,\alpha,\xi) &= f(w) + \sumitom \alpha_i g_i(w) \\
  &= \half \|w\|^2 + \frac{C}{2} \sumitom \xi_i^2 + \sumitom \alpha_i (-\yith (w^T \xith + b) + 1 - \xi_i) \\
  &= \half w^T w + \frac{C}{2} \xi^T \xi + \sumitom \alpha_i (-\yith w^T \xith - \yith b + 1 - \xi_i) \\
\end{align*}

\subsection*{c}

Minimize the Lagrangian with respect to $w$, $b$, and $\xi$ by taking the following gradients: $\nabla_w \mathcal{L}$, $\pder{\mathcal{L}}{b}$, and $\nabla_{\xi} \mathcal{L}$, and then setting them equal to $0$. Here $\xi = |\xi_1,\xi_2,...,\xi_m|^{T}$.

\subsection*{d}

What is the dual of the $\ell_2$ soft margin SVM optimization problem?

\end{document}
